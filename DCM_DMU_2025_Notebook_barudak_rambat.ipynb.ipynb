{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BANKING DEPOSIT SUBSCRIPTION PREDICTION\n",
      "============================================================\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 13:24:36,190] A new study created in memory with name: no-name-a0e73362-9d37-417d-a1ad-fd950d66319d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset Info:\n",
      "Rows: 22916, Features: 27\n",
      "Class Distribution:\n",
      "{0: 20302, 1: 2614}\n",
      "\n",
      "Training split: 18332 samples\n",
      "Validation split: 4584 samples\n",
      "\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b3ab754b154f65aa18f04c0d67f98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 13:24:43,716] Trial 0 finished with value: 0.7873198239612261 and parameters: {'algorithm': 'lgb', 'n_estimators': 679, 'max_depth': 4, 'learning_rate': 0.015957084694148364, 'subsample': 0.6232334448672797, 'colsample_bytree': 0.9464704583099741, 'reg_alpha': 0.02537815508265665, 'reg_lambda': 0.06796578090758151, 'num_leaves': 19, 'min_child_samples': 98, 'sampling': 'smote'}. Best is trial 0 with value: 0.7873198239612261.\n",
      "[I 2025-05-28 13:25:06,350] Trial 1 finished with value: 0.7899741515194466 and parameters: {'algorithm': 'cat', 'iterations': 545, 'depth': 4, 'learning_rate': 0.06252287916406217, 'l2_leaf_reg': 0.0004982752357076451, 'sampling': None}. Best is trial 1 with value: 0.7899741515194466.\n",
      "[I 2025-05-28 13:25:11,287] Trial 2 finished with value: 0.7896606166884265 and parameters: {'algorithm': 'xgb', 'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.061721159481070736, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'reg_alpha': 0.6245139574743068, 'reg_lambda': 0.7286653737491037, 'scale_pos_weight': 16.35954961421276, 'sampling': None}. Best is trial 1 with value: 0.7899741515194466.\n",
      "[I 2025-05-28 13:25:28,715] Trial 3 finished with value: 0.7947043198079584 and parameters: {'algorithm': 'cat', 'iterations': 227, 'depth': 8, 'learning_rate': 0.02171103454376615, 'l2_leaf_reg': 0.20540519425388448, 'sampling': None}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:25:48,335] Trial 4 finished with value: 0.7540499496232231 and parameters: {'algorithm': 'lgb', 'n_estimators': 952, 'max_depth': 9, 'learning_rate': 0.059963338824126605, 'subsample': 0.9687496940092467, 'colsample_bytree': 0.6353970008207678, 'reg_alpha': 0.0006080390190296605, 'reg_lambda': 0.00015167330688076205, 'num_leaves': 93, 'min_child_samples': 51, 'sampling': 'adasyn'}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:25:54,730] Trial 5 finished with value: 0.7734354854715229 and parameters: {'algorithm': 'lgb', 'n_estimators': 842, 'max_depth': 3, 'learning_rate': 0.19229567074543377, 'subsample': 0.908897907718663, 'colsample_bytree': 0.679486272613669, 'reg_alpha': 0.00010521761868451144, 'reg_lambda': 0.18274508859816008, 'num_leaves': 185, 'min_child_samples': 79, 'sampling': 'smote'}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:25:59,163] Trial 6 finished with value: 0.7854680253529435 and parameters: {'algorithm': 'lgb', 'n_estimators': 465, 'max_depth': 3, 'learning_rate': 0.02538617845650057, 'subsample': 0.7300733288106989, 'colsample_bytree': 0.8918424713352255, 'reg_alpha': 0.035500125258511595, 'reg_lambda': 0.35387588647792356, 'num_leaves': 128, 'min_child_samples': 29, 'sampling': 'adasyn'}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:26:04,273] Trial 7 finished with value: 0.7938640605195485 and parameters: {'algorithm': 'xgb', 'n_estimators': 542, 'max_depth': 3, 'learning_rate': 0.013815607382950859, 'subsample': 0.6125716742746937, 'colsample_bytree': 0.8545641645055122, 'reg_alpha': 0.0018089390092767149, 'reg_lambda': 0.010821382910613988, 'scale_pos_weight': 18.243763004595767, 'sampling': None}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:26:52,254] Trial 8 finished with value: 0.7580034904809526 and parameters: {'algorithm': 'cat', 'iterations': 329, 'depth': 8, 'learning_rate': 0.1125612361593632, 'l2_leaf_reg': 0.146898980776488, 'sampling': 'smote'}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:27:05,545] Trial 9 finished with value: 0.7887701418475468 and parameters: {'algorithm': 'xgb', 'n_estimators': 917, 'max_depth': 5, 'learning_rate': 0.013905315749737282, 'subsample': 0.6911740650167767, 'colsample_bytree': 0.7708431154505025, 'reg_alpha': 0.18709365688887356, 'reg_lambda': 0.27728241828010597, 'scale_pos_weight': 1.1320904800926233, 'sampling': 'smote'}. Best is trial 3 with value: 0.7947043198079584.\n",
      "[I 2025-05-28 13:27:22,122] Trial 10 finished with value: 0.7952333474985991 and parameters: {'algorithm': 'cat', 'iterations': 204, 'depth': 8, 'learning_rate': 0.02989346715281991, 'l2_leaf_reg': 7.113960632237399, 'sampling': None}. Best is trial 10 with value: 0.7952333474985991.\n",
      "[I 2025-05-28 13:27:37,739] Trial 11 finished with value: 0.7954959071253034 and parameters: {'algorithm': 'cat', 'iterations': 208, 'depth': 8, 'learning_rate': 0.02868955275514092, 'l2_leaf_reg': 9.408256681187533, 'sampling': None}. Best is trial 11 with value: 0.7954959071253034.\n",
      "[I 2025-05-28 13:28:30,352] Trial 12 finished with value: 0.7922101816953468 and parameters: {'algorithm': 'cat', 'iterations': 913, 'depth': 7, 'learning_rate': 0.03262210139878572, 'l2_leaf_reg': 9.459097445222497, 'sampling': None}. Best is trial 11 with value: 0.7954959071253034.\n",
      "[I 2025-05-28 13:28:42,287] Trial 13 finished with value: 0.7960836383782552 and parameters: {'algorithm': 'cat', 'iterations': 205, 'depth': 6, 'learning_rate': 0.039101431092308694, 'l2_leaf_reg': 9.68120997618957, 'sampling': None}. Best is trial 13 with value: 0.7960836383782552.\n",
      "[I 2025-05-28 13:29:03,102] Trial 14 finished with value: 0.7976431554206801 and parameters: {'algorithm': 'cat', 'iterations': 443, 'depth': 5, 'learning_rate': 0.04016324043483597, 'l2_leaf_reg': 1.7178317166633648, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:29:29,503] Trial 15 finished with value: 0.7970686583299258 and parameters: {'algorithm': 'cat', 'iterations': 506, 'depth': 5, 'learning_rate': 0.044545057173663756, 'l2_leaf_reg': 0.5131286147595261, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:30:01,861] Trial 16 finished with value: 0.7874348342354123 and parameters: {'algorithm': 'cat', 'iterations': 528, 'depth': 4, 'learning_rate': 0.09222919597676123, 'l2_leaf_reg': 0.3218054731855487, 'sampling': 'adasyn'}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:30:36,480] Trial 17 finished with value: 0.7897335618058511 and parameters: {'algorithm': 'cat', 'iterations': 723, 'depth': 5, 'learning_rate': 0.0454337133585062, 'l2_leaf_reg': 0.006288815567084455, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:30:53,631] Trial 18 finished with value: 0.7952977773146126 and parameters: {'algorithm': 'cat', 'iterations': 428, 'depth': 3, 'learning_rate': 0.09597295441760152, 'l2_leaf_reg': 0.9605268947495869, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:31:01,483] Trial 19 finished with value: 0.774968527554935 and parameters: {'algorithm': 'xgb', 'n_estimators': 212, 'max_depth': 8, 'learning_rate': 0.020576104727548856, 'subsample': 0.8250051428101366, 'colsample_bytree': 0.7433917487583578, 'reg_alpha': 0.003185785376801693, 'reg_lambda': 0.00012849200489000983, 'scale_pos_weight': 5.904837363937018, 'sampling': 'adasyn'}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:31:33,087] Trial 20 finished with value: 0.7875786596782104 and parameters: {'algorithm': 'cat', 'iterations': 700, 'depth': 5, 'learning_rate': 0.04573640318416906, 'l2_leaf_reg': 0.0151530190027151, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:31:54,540] Trial 21 finished with value: 0.7963684440127512 and parameters: {'algorithm': 'cat', 'iterations': 407, 'depth': 6, 'learning_rate': 0.038452972453134636, 'l2_leaf_reg': 1.2048195701099635, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:32:17,761] Trial 22 finished with value: 0.7963342051244681 and parameters: {'algorithm': 'cat', 'iterations': 416, 'depth': 6, 'learning_rate': 0.04063632942311978, 'l2_leaf_reg': 0.9413403416177035, 'sampling': None}. Best is trial 14 with value: 0.7976431554206801.\n",
      "[I 2025-05-28 13:32:38,389] Trial 23 finished with value: 0.7979350236407087 and parameters: {'algorithm': 'cat', 'iterations': 430, 'depth': 5, 'learning_rate': 0.05182810610563572, 'l2_leaf_reg': 1.213994527279806, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:33:06,158] Trial 24 finished with value: 0.7899181160703341 and parameters: {'algorithm': 'cat', 'iterations': 602, 'depth': 4, 'learning_rate': 0.08043886422717914, 'l2_leaf_reg': 0.045968993556054355, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:33:32,432] Trial 25 finished with value: 0.7950427885883442 and parameters: {'algorithm': 'cat', 'iterations': 501, 'depth': 5, 'learning_rate': 0.010354263660840603, 'l2_leaf_reg': 2.0431760510156174, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:33:49,259] Trial 26 finished with value: 0.7831166496265822 and parameters: {'algorithm': 'cat', 'iterations': 338, 'depth': 5, 'learning_rate': 0.1392953011089294, 'l2_leaf_reg': 0.08330155295269051, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:34:19,028] Trial 27 finished with value: 0.7924361419391944 and parameters: {'algorithm': 'cat', 'iterations': 650, 'depth': 4, 'learning_rate': 0.07027942056748941, 'l2_leaf_reg': 0.38601045931826955, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:34:25,633] Trial 28 finished with value: 0.7832959503855366 and parameters: {'algorithm': 'lgb', 'n_estimators': 258, 'max_depth': 7, 'learning_rate': 0.052587656102807134, 'subsample': 0.7915017490929934, 'colsample_bytree': 0.9839442177063762, 'reg_alpha': 0.00027468587754730147, 'reg_lambda': 0.0012424791659069155, 'num_leaves': 255, 'min_child_samples': 21, 'sampling': 'adasyn'}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:34:31,335] Trial 29 finished with value: 0.7673291256306806 and parameters: {'algorithm': 'xgb', 'n_estimators': 326, 'max_depth': 6, 'learning_rate': 0.05189429889427863, 'subsample': 0.9997653976350742, 'colsample_bytree': 0.8359453507286072, 'reg_alpha': 0.12331893461755762, 'reg_lambda': 0.010077797849245958, 'scale_pos_weight': 11.194893305541884, 'sampling': 'smote'}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:34:36,670] Trial 30 finished with value: 0.7895887748019309 and parameters: {'algorithm': 'lgb', 'n_estimators': 418, 'max_depth': 9, 'learning_rate': 0.03391505809131767, 'subsample': 0.8872533273383123, 'colsample_bytree': 0.7212380647249992, 'reg_alpha': 0.0069802884074791245, 'reg_lambda': 0.0014149506384703333, 'num_leaves': 18, 'min_child_samples': 56, 'sampling': 'smote'}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:34:58,368] Trial 31 finished with value: 0.797028303495984 and parameters: {'algorithm': 'cat', 'iterations': 428, 'depth': 6, 'learning_rate': 0.037103508112509995, 'l2_leaf_reg': 1.8372393761913308, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:35:22,727] Trial 32 finished with value: 0.7970830239520053 and parameters: {'algorithm': 'cat', 'iterations': 472, 'depth': 6, 'learning_rate': 0.0248996614728298, 'l2_leaf_reg': 2.6073400371028046, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:35:45,252] Trial 33 finished with value: 0.796663147531061 and parameters: {'algorithm': 'cat', 'iterations': 502, 'depth': 5, 'learning_rate': 0.01854739515512124, 'l2_leaf_reg': 2.6574806611083885, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:36:04,718] Trial 34 finished with value: 0.7963119971438684 and parameters: {'algorithm': 'cat', 'iterations': 332, 'depth': 7, 'learning_rate': 0.025384645790828175, 'l2_leaf_reg': 0.5442495109416903, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:36:39,867] Trial 35 finished with value: 0.7850683309270797 and parameters: {'algorithm': 'cat', 'iterations': 584, 'depth': 7, 'learning_rate': 0.05601926547199506, 'l2_leaf_reg': 2.996745258394963, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:37:01,335] Trial 36 finished with value: 0.7868876610972609 and parameters: {'algorithm': 'cat', 'iterations': 453, 'depth': 5, 'learning_rate': 0.07080223339503726, 'l2_leaf_reg': 0.00011265285296945432, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:37:06,448] Trial 37 finished with value: 0.7902071234208061 and parameters: {'algorithm': 'lgb', 'n_estimators': 789, 'max_depth': 6, 'learning_rate': 0.02503974239354553, 'subsample': 0.7684840275664269, 'colsample_bytree': 0.9076150904385443, 'reg_alpha': 0.0012256393822435885, 'reg_lambda': 0.0409856625336691, 'num_leaves': 243, 'min_child_samples': 99, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:37:12,565] Trial 38 finished with value: 0.7781382041779981 and parameters: {'algorithm': 'xgb', 'n_estimators': 605, 'max_depth': 5, 'learning_rate': 0.04692003406039711, 'subsample': 0.8721584140290084, 'colsample_bytree': 0.6003939968101376, 'reg_alpha': 0.6469059185873418, 'reg_lambda': 0.0006005107105510226, 'scale_pos_weight': 11.368609760828178, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:37:41,014] Trial 39 finished with value: 0.7917256426188186 and parameters: {'algorithm': 'cat', 'iterations': 370, 'depth': 6, 'learning_rate': 0.01665252106194926, 'l2_leaf_reg': 0.49926185331953865, 'sampling': 'adasyn'}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:38:04,297] Trial 40 finished with value: 0.7926736835589252 and parameters: {'algorithm': 'cat', 'iterations': 471, 'depth': 3, 'learning_rate': 0.0627396128794091, 'l2_leaf_reg': 4.3151447270168815, 'sampling': 'smote'}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:38:23,874] Trial 41 finished with value: 0.7962629914374622 and parameters: {'algorithm': 'cat', 'iterations': 388, 'depth': 6, 'learning_rate': 0.03424173991715392, 'l2_leaf_reg': 1.5607367243702235, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:38:38,859] Trial 42 finished with value: 0.7959380604291453 and parameters: {'algorithm': 'cat', 'iterations': 290, 'depth': 6, 'learning_rate': 0.021875366524836667, 'l2_leaf_reg': 3.3416062010044567, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:38:59,355] Trial 43 finished with value: 0.7977572544659515 and parameters: {'algorithm': 'cat', 'iterations': 460, 'depth': 5, 'learning_rate': 0.029595971688468465, 'l2_leaf_reg': 0.6871889420702106, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:39:24,745] Trial 44 finished with value: 0.796010789841284 and parameters: {'algorithm': 'cat', 'iterations': 567, 'depth': 5, 'learning_rate': 0.02919534347617577, 'l2_leaf_reg': 0.0017610201373104534, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n",
      "[I 2025-05-28 13:39:27,927] Trial 45 finished with value: 0.7919770561332491 and parameters: {'algorithm': 'lgb', 'n_estimators': 376, 'max_depth': 7, 'learning_rate': 0.027178637869536736, 'subsample': 0.9463308959871887, 'colsample_bytree': 0.9994563730237144, 'reg_alpha': 0.018699186484878925, 'reg_lambda': 0.004099825586685666, 'num_leaves': 71, 'min_child_samples': 73, 'sampling': None}. Best is trial 23 with value: 0.7979350236407087.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def create_enhanced_features(data):\n",
    "    \"\"\"Sophisticated feature engineering for banking domain\"\"\"\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Contact history features\n",
    "    data['previous_contact_flag'] = np.where(data['hari_sejak_kontak_sebelumnya'] == 999, 0, 1)\n",
    "    data['success_ratio'] = (\n",
    "        data['hasil_kampanye_sebelumnya'].map({'success': 1, 'failure': 0, 'nonexistent': 0}) * \n",
    "        (data['jumlah_kontak_sebelumnya'] + 1)\n",
    "    )\n",
    "    \n",
    "    # Economic composite features\n",
    "    economic_features = ['indeks_harga_konsumen', 'suku_bunga_euribor_3bln', 'tingkat_variasi_pekerjaan']\n",
    "    scaler = StandardScaler()\n",
    "    data[economic_features] = scaler.fit_transform(data[economic_features])\n",
    "    data['economic_risk_score'] = (0.4 * data['indeks_harga_konsumen'] + \n",
    "                                  0.4 * data['suku_bunga_euribor_3bln'] + \n",
    "                                  0.2 * data['tingkat_variasi_pekerjaan'])\n",
    "    \n",
    "    # Demographic features\n",
    "    age_bins = [0, 25, 35, 45, 55, 65, 100]\n",
    "    data['age_group'] = pd.cut(data['usia'], bins=age_bins, \n",
    "                              labels=['18-25', '26-35', '36-45', '46-55', '56-65', '66+'])\n",
    "    \n",
    "    # Loan features\n",
    "    loan_mapping = {'yes': 1, 'no': 0, 'unknown': 0.5}\n",
    "    data['housing_loan'] = data['pinjaman_rumah'].map(loan_mapping)\n",
    "    data['personal_loan'] = data['pinjaman_pribadi'].map(loan_mapping)\n",
    "    \n",
    "    # Temporal features\n",
    "    month_map = {'jan':1, 'feb':2, 'mar':3, 'apr':4, 'may':5, 'jun':6,\n",
    "                'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov':11, 'dec':12}\n",
    "    data['contact_month'] = data['bulan_kontak_terakhir'].map(month_map)\n",
    "    data['quarter'] = pd.cut(data['contact_month'], \n",
    "                            bins=[0,3,6,9,12], \n",
    "                            labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "    \n",
    "    # Interaction features\n",
    "    data['contact_success_ratio'] = data['jumlah_kontak_kampanye_ini'] * data['success_ratio']\n",
    "    data['age_economic_interaction'] = data['usia'] * data['economic_risk_score']\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_advanced_preprocessor():\n",
    "    \"\"\"Create robust preprocessing pipeline with proper categorical handling\"\"\"\n",
    "    \n",
    "    categorical_features = [\n",
    "        'pekerjaan', 'status_perkawinan', 'pendidikan', 'gagal_bayar_sebelumnya',\n",
    "        'pinjaman_rumah', 'pinjaman_pribadi', 'jenis_kontak', 'quarter',\n",
    "        'hasil_kampanye_sebelumnya', 'pulau', 'age_group'\n",
    "    ]\n",
    "    \n",
    "    numerical_features = [\n",
    "        'usia', 'jumlah_kontak_kampanye_ini', 'hari_sejak_kontak_sebelumnya',\n",
    "        'jumlah_kontak_sebelumnya', 'indeks_harga_konsumen',\n",
    "        'indeks_kepercayaan_konsumen', 'suku_bunga_euribor_3bln',\n",
    "        'jumlah_pekerja', 'previous_contact_flag', 'success_ratio',\n",
    "        'economic_risk_score', 'housing_loan', 'personal_loan',\n",
    "        'contact_month', 'contact_success_ratio', 'age_economic_interaction'\n",
    "    ]\n",
    "\n",
    "    numerical_pipe = Pipeline([\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipe = Pipeline([\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_pipe, numerical_features),\n",
    "        ('cat', categorical_pipe, categorical_features)\n",
    "    ], remainder='drop')\n",
    "\n",
    "    return preprocessor, numerical_features + categorical_features\n",
    "\n",
    "def enhanced_objective(trial, X, y, preprocessor):\n",
    "    \"\"\"Optimization objective with proper pipeline integration\"\"\"\n",
    "    \n",
    "    algorithm = trial.suggest_categorical(\"algorithm\", [\"xgb\", \"lgb\", \"cat\"])\n",
    "    \n",
    "    # Algorithm configuration\n",
    "    if algorithm == \"xgb\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 3, 9),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-4, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-4, 1.0, log=True),\n",
    "            'scale_pos_weight': trial.suggest_float(\"scale_pos_weight\", 1, 20),\n",
    "            'random_state': 42,\n",
    "            'eval_metric': 'auc',\n",
    "        }\n",
    "        model = XGBClassifier(**params)\n",
    "    elif algorithm == \"lgb\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 3, 9),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-4, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-4, 1.0, log=True),\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "            'min_child_samples': trial.suggest_int(\"min_child_samples\", 20, 100),\n",
    "            'random_state': 42,\n",
    "            'verbosity': -1,\n",
    "        }\n",
    "        model = LGBMClassifier(**params)\n",
    "    else:\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int(\"iterations\", 200, 1000),\n",
    "            'depth': trial.suggest_int(\"depth\", 3, 8),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 1e-4, 10.0, log=True),\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "        }\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "    # Sampling strategy\n",
    "    sampler = trial.suggest_categorical(\"sampling\", [\"smote\", \"adasyn\", None])\n",
    "    if sampler == \"smote\":\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampler == \"adasyn\":\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        # Create complete pipeline\n",
    "        pipeline_steps = [('preprocessor', preprocessor)]\n",
    "        if sampler:\n",
    "            pipeline_steps.append(('sampler', sampler))\n",
    "        pipeline_steps.append(('model', model))\n",
    "        \n",
    "        pipeline = ImbPipeline(pipeline_steps)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = pipeline.predict_proba(X_valid)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_valid, y_pred))\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "def train_optimized_model():\n",
    "    \"\"\"Train the model using Optuna with train/validation split\"\"\"\n",
    "    \n",
    "    print(\"Loading training data...\")\n",
    "    df_train = pd.read_csv(\"https://raw.githubusercontent.com/difadlyaulhaq/junk/refs/heads/main/training_dataset.csv\")\n",
    "    df_train = create_enhanced_features(df_train)\n",
    "    \n",
    "    preprocessor, features = create_advanced_preprocessor()\n",
    "    X = df_train[features]\n",
    "    y = df_train['berlangganan_deposito']\n",
    "    \n",
    "    print(f\"\\nTraining Dataset Info:\\nRows: {X.shape[0]}, Features: {X.shape[1]}\")\n",
    "    print(f\"Class Distribution:\\n{y.value_counts().to_dict()}\\n\")\n",
    "    \n",
    "    # Split data for internal validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training split: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation split: {X_val.shape[0]} samples\")\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    print(\"\\nStarting hyperparameter optimization...\")\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(lambda trial: enhanced_objective(trial, X_train, y_train, preprocessor), \n",
    "                   n_trials=50, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nBest Trial:\")\n",
    "    print(f\"AUC: {study.best_value:.4f}\")\n",
    "    print(f\"Params: {study.best_params}\")\n",
    "    \n",
    "    # Train final model on full training data\n",
    "    best_params = study.best_params.copy()\n",
    "    algorithm = best_params.pop('algorithm')\n",
    "    sampling = best_params.pop('sampling')\n",
    "    \n",
    "    # Select the best model\n",
    "    if algorithm == \"xgb\":\n",
    "        model = XGBClassifier(**best_params, random_state=42, eval_metric='auc')\n",
    "    elif algorithm == \"lgb\":\n",
    "        model = LGBMClassifier(**best_params, random_state=42, verbosity=-1)\n",
    "    else:\n",
    "        model = CatBoostClassifier(**best_params, random_seed=42, verbose=False)\n",
    "    \n",
    "    if sampling == \"smote\":\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif sampling == \"adasyn\":\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    pipeline_steps = [('preprocessor', preprocessor)]\n",
    "    if sampler:\n",
    "        pipeline_steps.append(('sampler', sampler))\n",
    "    pipeline_steps.append(('model', model))\n",
    "    final_pipeline = ImbPipeline(pipeline_steps)\n",
    "\n",
    "    print(\"\\nFitting final model on full training data...\")\n",
    "    final_pipeline.fit(X, y)\n",
    "    \n",
    "    # Validate on internal validation set\n",
    "    print(\"\\nValidating on internal validation set...\")\n",
    "    y_pred_prob = final_pipeline.predict_proba(X_val)[:, 1]\n",
    "    y_pred_label = final_pipeline.predict(X_val)\n",
    "    \n",
    "    auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "    print(f\"Internal Validation AUC: {auc_score:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred_label))\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve on Internal Validation Set')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return final_pipeline, features, study\n",
    "\n",
    "def make_predictions_on_validation_set(model, features):\n",
    "    \"\"\"Generate predictions for the validation set (test data)\"\"\"\n",
    "    \n",
    "    print(\"Loading validation set for predictions...\")\n",
    "    df_val = pd.read_csv(\"https://raw.githubusercontent.com/difadlyaulhaq/junk/refs/heads/main/validation_set.csv\")\n",
    "    df_val = create_enhanced_features(df_val)\n",
    "    \n",
    "    # Handle feature mismatch\n",
    "    available_features = [f for f in features if f in df_val.columns]\n",
    "    missing = list(set(features) - set(available_features))\n",
    "    if missing:\n",
    "        print(f\"Warning: Missing features in validation data: {missing}\")\n",
    "        # Remove missing features from the feature list for prediction\n",
    "        features = available_features\n",
    "    \n",
    "    X_val = df_val[features]\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'customer_number': df_val['customer_number'],\n",
    "        'berlangganan_deposito': y_pred_prob\n",
    "    }).sort_values('customer_number')\n",
    "    \n",
    "    submission.to_csv(\"DCM_DMU_2025_Model_barudak_rambat.csv\", index=False)\n",
    "    print(f\"\\nPredictions saved to DCM_DMU_2025_Model_barudak_rambat.csv\")\n",
    "    print(f\"Submission shape: {submission.shape}\")\n",
    "    print(f\"Prediction range: [{y_pred_prob.min():.4f}, {y_pred_prob.max():.4f}]\")\n",
    "    print(f\"Mean prediction: {y_pred_prob.mean():.4f}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "def evaluate_if_labels_available(model, features):\n",
    "    \"\"\"Evaluate model if validation set has labels\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_val = pd.read_csv(\"validation_set.csv\")\n",
    "        if 'berlangganan_deposito' not in df_val.columns:\n",
    "            print(\"No target labels in validation set - skipping evaluation\")\n",
    "            return None\n",
    "            \n",
    "        df_val = create_enhanced_features(df_val)\n",
    "        X_val = df_val[features]\n",
    "        y_val = df_val['berlangganan_deposito']\n",
    "        \n",
    "        y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred_label = model.predict(X_val)\n",
    "        \n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(recall, precision, label='PR Curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nValidation Set AUC: {auc_score:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred_label))\n",
    "        \n",
    "        return auc_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not evaluate on validation set: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_feature_importance(model, features):\n",
    "    \"\"\"Analyze and plot feature importance\"\"\"\n",
    "    \n",
    "    # Get the trained model from the pipeline\n",
    "    trained_model = model.named_steps['model']\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        importance = trained_model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot top 20 features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 20 Feature Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10).to_string(index=False))\n",
    "        \n",
    "        return feature_importance\n",
    "    else:\n",
    "        print(\"Model does not have feature_importances_ attribute\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"BANKING DEPOSIT SUBSCRIPTION PREDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train model\n",
    "    model, features, study = train_optimized_model()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best AUC: {study.best_value:.4f}\")\n",
    "    print(f\"Best Algorithm: {study.best_params.get('algorithm', 'Unknown')}\")\n",
    "    print(f\"Best Sampling: {study.best_params.get('sampling', 'None')}\")\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    feature_importance = analyze_feature_importance(model, features)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING PREDICTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    submission = make_predictions_on_validation_set(model, features)\n",
    "    \n",
    "    # Try to evaluate if labels are available\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION (if labels available)\")\n",
    "    print(\"=\"*60)\n",
    "    final_auc = evaluate_if_labels_available(model, features)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Files generated:\")\n",
    "    print(\"- DCM_DMU_2025_Model_barudak_rambat.csv      : Predictions for validation set\")\n",
    "    if final_auc:\n",
    "        print(f\"- Final validation AUC: {final_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
